{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data and necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ismathakit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ismathakit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ismathakit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ismathakit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textstat import flesch_kincaid_grade\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/ismathakit/Downloads/trainingandtestdata/train.csv\", encoding_errors='ignore')\n",
    "test_df = pd.read_csv(\"/Users/ismathakit/Downloads/trainingandtestdata/test.csv\", encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore dataset separately to test and train.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Date      Type       Username  \\\n",
       "0  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                Text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.iloc[:,2:]\n",
    "train_df.columns = ['Date', 'Type', 'Username', 'Text']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two columns doesn't tell any significances for the information. So, I'd like to drop those of according to the train header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon May 11 03:22:00 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>GeorgeVHulme</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Date     Type      Username  \\\n",
       "0  Mon May 11 03:18:03 UTC 2009  kindle2        vcu451   \n",
       "1  Mon May 11 03:18:54 UTC 2009  kindle2        chadfu   \n",
       "2  Mon May 11 03:19:04 UTC 2009  kindle2         SIX15   \n",
       "3  Mon May 11 03:21:41 UTC 2009  kindle2      yamarama   \n",
       "4  Mon May 11 03:22:00 UTC 2009  kindle2  GeorgeVHulme   \n",
       "\n",
       "                                                Text  \n",
       "0  Reading my kindle2...  Love it... Lee childs i...  \n",
       "1  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "2  @kenburbary You'll love your Kindle2. I've had...  \n",
       "3  @mikefish  Fair enough. But i have the Kindle2...  \n",
       "4  @richardebaker no. it is too big. I'm quite ha...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New test data\n",
    "test_df = test_df.iloc[:,2:]\n",
    "test_df.columns = ['Date', 'Type', 'Username', 'Text']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "Remove URLs, mentions, hashtags, numbers, and special characters using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing URLs, mentions, hashtags, numbers, and special characters.\"\"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.strip()  # Strip leading/trailing whitespaces\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text into words.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from tokenized text.\"\"\"\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def stem_text(tokens):\n",
    "    \"\"\"Apply stemming to tokens.\"\"\"\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    \"\"\"Apply lemmatization to tokens.\"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "'''\n",
    "def preprocess_data(df, text_column):\n",
    "    \"\"\"Preprocess the text data in the specified column of a dataframe.\"\"\"\n",
    "    df[\"Cleaned_Text\"] = df[text_column].apply(clean_text)\n",
    "    df[\"Tokens\"] = df[\"Cleaned_Text\"].apply(tokenize_text)\n",
    "    df[\"Tokens\"] = df[\"Tokens\"].apply(remove_stopwords)\n",
    "    df[\"Stemmed_Tokens\"] = df[\"Tokens\"].apply(stem_text)\n",
    "    df[\"Lemmatized_Tokens\"] = df[\"Tokens\"].apply(lemmatize_text)\n",
    "    df[\"Processed_Text\"] = df[\"Lemmatized_Tokens\"].apply(lambda x: \" \".join(x))\n",
    "'''\n",
    "def preprocess_data(df, text_column):\n",
    "    \"\"\"Preprocess the text data and add a 'Processed_Text' column to the dataframe.\"\"\"\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    # Ensure stopwords are available\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean individual text input.\"\"\"\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Stemming and Lemmatization\n",
    "        tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens]\n",
    "        # Rejoin tokens\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Apply cleaning function to the text column\n",
    "    df['Processed_Text'] = df[text_column].apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = preprocess_data(train_df, text_column='Text')  # Replace 'Text' with the correct column name in train.csv\n",
    "test_df = preprocess_data(test_df, text_column='Text')  # Replace 'Text' with the correct column name in test.csv\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  is upset that he can't update his Facebook by ...   \n",
      "1  @Kenichan I dived many times for the ball. Man...   \n",
      "2    my whole body feels itchy and like its on fire    \n",
      "3  @nationwideclass no, it's not behaving at all....   \n",
      "4                      @Kwesidei not the whole crew    \n",
      "\n",
      "                                      Processed_Text  \n",
      "0  upset cant updat facebook text might cri resul...  \n",
      "1  kenichan dive mani time ball manag save rest g...  \n",
      "2                    whole bodi feel itchi like fire  \n",
      "3              nationwideclass behav im mad cant see  \n",
      "4                                kwesidei whole crew  \n",
      "                                                Text  \\\n",
      "0  Reading my kindle2...  Love it... Lee childs i...   \n",
      "1  Ok, first assesment of the #kindle2 ...it fuck...   \n",
      "2  @kenburbary You'll love your Kindle2. I've had...   \n",
      "3  @mikefish  Fair enough. But i have the Kindle2...   \n",
      "4  @richardebaker no. it is too big. I'm quite ha...   \n",
      "\n",
      "                                      Processed_Text  \n",
      "0                read kindl love lee child good read  \n",
      "1                     ok first asses kindl fuck rock  \n",
      "2  kenburbari youll love kindl ive mine month nev...  \n",
      "3           mikefish fair enough kindl think perfect  \n",
      "4                richardebak big im quit happi kindl  \n"
     ]
    }
   ],
   "source": [
    "# Check if 'Processed_Text' was added successfully\n",
    "print(train_df[['Text', 'Processed_Text']].head())\n",
    "print(test_df[['Text', 'Processed_Text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it worked :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare basic statistics before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(text_series):\n",
    "    \"\"\"Calculate basic text statistics: average sentence length, word count, vocabulary size.\"\"\"\n",
    "    total_words = sum(text_series.apply(lambda x: len(x.split())))\n",
    "    unique_words = len(set(\" \".join(text_series).split()))\n",
    "    average_sentence_length = total_words / len(text_series)\n",
    "    return total_words, unique_words, average_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning (Train and Test)\n",
    "original_train_stats = calculate_statistics(train_df[\"Text\"])\n",
    "original_test_stats = calculate_statistics(test_df[\"Text\"])\n",
    "\n",
    "# After cleaning (Train and Test)\n",
    "cleaned_train_stats = calculate_statistics(train_df[\"Processed_Text\"])\n",
    "cleaned_test_stats = calculate_statistics(test_df[\"Processed_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate text quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexical_diversity(text_series):\n",
    "    \"\"\"Calculate lexical diversity (ratio of unique words to total words).\"\"\"\n",
    "    total_words = sum(text_series.apply(lambda x: len(x.split())))\n",
    "    unique_words = len(set(\" \".join(text_series).split()))\n",
    "    return unique_words / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Readability_Score\"] = train_df[\"Processed_Text\"].apply(flesch_kincaid_grade)\n",
    "test_df[\"Readability_Score\"] = test_df[\"Processed_Text\"].apply(flesch_kincaid_grade)\n",
    "\n",
    "train_lexical_diversity = calculate_lexical_diversity(train_df[\"Processed_Text\"])\n",
    "test_lexical_diversity = calculate_lexical_diversity(test_df[\"Processed_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train Dataset Cleaning Statistics ---\n",
      "- Number of documents: 1599999 → 1599999\n",
      "- Average tokens per document: 13.175500109687569 → 7.670459169036981\n",
      "- Total vocabulary size: 1350157 → 672984\n",
      "- Lexical diversity: 0.05\n",
      "- Average readability score (Flesch-Kincaid): 4.30\n"
     ]
    }
   ],
   "source": [
    "runtime = end_time - start_time\n",
    "\n",
    "print(\"--- Train Dataset Cleaning Statistics ---\")\n",
    "print(f\"- Number of documents: {len(train_df['Text'])} → {len(train_df['Processed_Text'])}\")\n",
    "print(f\"- Average tokens per document: {original_train_stats[2]} → {cleaned_train_stats[2]}\")\n",
    "print(f\"- Total vocabulary size: {original_train_stats[1]} → {cleaned_train_stats[1]}\")\n",
    "print(f\"- Lexical diversity: {train_lexical_diversity:.2f}\")\n",
    "print(f\"- Average readability score (Flesch-Kincaid): {train_df['Readability_Score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Dataset Cleaning Statistics ---\n",
      "- Number of documents: 497 → 497\n",
      "- Average tokens per document: 13.541247484909457 → 8.227364185110664\n",
      "- Total vocabulary size: 3150 → 1776\n",
      "- Lexical diversity: 0.43\n",
      "- Average readability score (Flesch-Kincaid): 4.75\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test Dataset Cleaning Statistics ---\")\n",
    "print(f\"- Number of documents: {len(test_df['Text'])} → {len(test_df['Processed_Text'])}\")\n",
    "print(f\"- Average tokens per document: {original_test_stats[2]} → {cleaned_test_stats[2]}\")\n",
    "print(f\"- Total vocabulary size: {original_test_stats[1]} → {cleaned_test_stats[1]}\")\n",
    "print(f\"- Lexical diversity: {test_lexical_diversity:.2f}\")\n",
    "print(f\"- Average readability score (Flesch-Kincaid): {test_df['Readability_Score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Total Runtime ---\n",
      "- Total runtime: 169.54 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Total Runtime ---\")\n",
    "print(f\"- Total runtime: {runtime:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
